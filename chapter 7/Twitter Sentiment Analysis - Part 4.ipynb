{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Part 4\n",
    "## Create a real-time dashboard PixieApp \n",
    "\n",
    "In this notebook, we scale the application by using Kafka and IBM Streaming Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment and run the line below to install tweepy if needed\n",
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up twitter authentication\n",
    "Make sure to fill in the tokens below before running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "# Go to http://apps.twitter.com and create an app.\n",
    "# The consumer key and secret will be generated for you after\n",
    "consumer_key=\"XXXX\"\n",
    "consumer_secret=\"XXXX\"\n",
    "\n",
    "# After the step above, you will be redirected to your app's page.\n",
    "# Create an access token under the the \"Your access token\" section\n",
    "access_token=\"XXXX\"\n",
    "access_token_secret=\"XXXX\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide the Message Hub service credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message_hub_creds = {\n",
    "  \"instance_id\": \"XXXX\",\n",
    "  \"mqlight_lookup_url\": \"https://mqlight-lookup-prod02.messagehub.services.us-south.bluemix.net/Lookup?serviceId=efb1eea8-8561-485f-8993-a760ece64c42\",\n",
    "  \"api_key\": \"XXXX\",\n",
    "  \"kafka_admin_url\": \"https://kafka-admin-prod02.messagehub.services.us-south.bluemix.net:443\",\n",
    "  \"kafka_rest_url\": \"https://kafka-rest-prod02.messagehub.services.us-south.bluemix.net:443\",\n",
    "  \"kafka_brokers_sasl\": [\n",
    "    \"kafka03-prod02.messagehub.services.us-south.bluemix.net:9093\",\n",
    "    \"kafka01-prod02.messagehub.services.us-south.bluemix.net:9093\",\n",
    "    \"kafka02-prod02.messagehub.services.us-south.bluemix.net:9093\",\n",
    "    \"kafka05-prod02.messagehub.services.us-south.bluemix.net:9093\",\n",
    "    \"kafka04-prod02.messagehub.services.us-south.bluemix.net:9093\"\n",
    "  ],\n",
    "  \"user\": \"XXXX\",\n",
    "  \"password\": \"XXXX\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper method to create a kafka topic is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def ensure_topic_exists(topic_name):\n",
    "    response = requests.post(\n",
    "                message_hub_creds[\"kafka_rest_url\"] + \"/admin/topics\", \n",
    "                data = json.dumps({\"name\": topic_name}),\n",
    "                headers={\"X-Auth-Token\": message_hub_creds[\"api_key\"]}\n",
    "            )\n",
    "    if response.status_code != 200 and response.status_code != 202 and \\\n",
    "       response.status_code != 422 and response.status_code != 403:\n",
    "        raise Exception(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Twitter Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.9</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from pixiedust.utils import Logger\n",
    "from tweepy import Stream\n",
    "from six import iteritems\n",
    "import json\n",
    "import csv\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, TimestampType\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import ssl\n",
    "\n",
    "def ensure_dir(dir, delete_tree = False):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    elif delete_tree:\n",
    "        shutil.rmtree(dir)\n",
    "        os.makedirs(dir)\n",
    "    return os.path.abspath(dir)\n",
    "\n",
    "def init_output_dirs():\n",
    "    root_dir = ensure_dir(\"output\", delete_tree = True)\n",
    "    output_dir = ensure_dir(os.path.join(root_dir, \"raw\"))\n",
    "    return (root_dir, output_dir)\n",
    "    \n",
    "root_dir, output_dir = init_output_dirs()\n",
    "\n",
    "field_metadata = [\n",
    "    {\"name\": \"created_at\",\"type\": TimestampType()},\n",
    "    {\"name\": \"text\", \"type\": StringType()},\n",
    "    {\"name\": \"source\", \"type\": StringType(), \n",
    "         \"transform\": lambda s: BS(s, \"html.parser\").text.strip()\n",
    "    },\n",
    "    {\"name\": \"sentiment\", \"type\": StringType()},\n",
    "    {\"name\": \"entity\", \"type\": StringType()},\n",
    "    {\"name\": \"entity_type\", \"type\": StringType()}\n",
    "]\n",
    "fieldnames = [f[\"name\"] for f in field_metadata]\n",
    "transforms = { \n",
    "    item['name']:item['transform'] for item in field_metadata if \"transform\" in item\n",
    "}\n",
    "\n",
    "@Logger()\n",
    "class RawTweetsListener(StreamListener):\n",
    "    def __init__(self):\n",
    "        self.buffered_data = []\n",
    "        self.counter = 0\n",
    "        self.tweet_count = 0\n",
    "        \n",
    "        self.topic = \"tweets\"\n",
    "        ensure_topic_exists(self.topic)\n",
    "        context = ssl.create_default_context()\n",
    "        context.options &= ssl.OP_NO_TLSv1\n",
    "        context.options &= ssl.OP_NO_TLSv1_1\n",
    "        kafka_conf = {\n",
    "            'sasl_mechanism': 'PLAIN',\n",
    "            'security_protocol': 'SASL_SSL',\n",
    "            'ssl_context': context,\n",
    "            \"bootstrap_servers\": message_hub_creds[\"kafka_brokers_sasl\"],\n",
    "            \"sasl_plain_username\": message_hub_creds[\"user\"],\n",
    "            \"sasl_plain_password\": message_hub_creds[\"password\"],\n",
    "            \"api_version\":(0, 10, 1),\n",
    "            \"value_serializer\" : lambda v: json.dumps(v).encode('utf-8')\n",
    "        }\n",
    "        self.producer = KafkaProducer(**kafka_conf)\n",
    "\n",
    "    def on_data(self, data):\n",
    "        def transform(key, value):\n",
    "            return transforms[key](value) if key in transforms else value\n",
    "        self.tweet_count += 1\n",
    "        self.producer.send(\n",
    "            self.topic, \n",
    "            {key:transform(key,value) \\\n",
    "                 for key,value in iteritems(json.loads(data)) \\\n",
    "                 if key in fieldnames}\n",
    "        )\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(\"An error occured while receiving streaming data: {}\".format(status))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_stream(queries):\n",
    "    \"Asynchronously start a new Twitter stream\"\n",
    "    stream = Stream(auth, RawTweetsListener())\n",
    "    stream.filter(track=queries, languages=[\"en\"], async=True)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark Streaming DataFrame using Kafka as the Input Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def start_streaming_dataframe():\n",
    "    \"Start a Spark Streaming DataFrame from a Kafka Input source\"\n",
    "    schema = StructType(\n",
    "        [StructField(f[\"name\"], f[\"type\"], True) for f in field_metadata]\n",
    "    )\n",
    "    kafka_options = {\n",
    "        \"kafka.ssl.protocol\":\"TLSv1.2\",\n",
    "        \"kafka.ssl.enabled.protocols\":\"TLSv1.2\",\n",
    "        \"kafka.ssl.endpoint.identification.algorithm\":\"HTTPS\",\n",
    "        'kafka.sasl.mechanism': 'PLAIN',\n",
    "        'kafka.security.protocol': 'SASL_SSL'\n",
    "    }\n",
    "    return spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \",\".join(message_hub_creds[\"kafka_brokers_sasl\"])) \\\n",
    "        .option(\"subscribe\", \"enriched_tweets\") \\\n",
    "        .load(**kafka_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Run Spark Structured Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_parquet_streaming_query(csv_sdf):\n",
    "    \"\"\"\n",
    "    Create an run a streaming query from a Structured DataFrame \n",
    "    outputing the results into a parquet database\n",
    "    \"\"\"\n",
    "    streaming_query = csv_sdf \\\n",
    "      .writeStream \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", os.path.join(root_dir, \"output_parquet\")) \\\n",
    "      .trigger(processingTime=\"10 seconds\") \\\n",
    "      .option(\"checkpointLocation\", os.path.join(root_dir, \"output_chkpt\")) \\\n",
    "      .start()\n",
    "    return streaming_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a real-time dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StreamsManager class for controlling the lifecycle of the different streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StreamsManager():\n",
    "    def __init__(self):\n",
    "        self.twitter_stream = None\n",
    "        self.csv_sdf = None\n",
    "        \n",
    "    def reset(self, search_query = None):\n",
    "        if self.twitter_stream is not None:\n",
    "            self.twitter_stream.disconnect()\n",
    "        #stop all the active streaming queries and re_initialize the directories\n",
    "        for query in spark.streams.active:\n",
    "            query.stop()\n",
    "        self.root_dir, self.output_dir = init_output_dirs()\n",
    "        self.twitter_stream = start_stream([search_query]) if search_query is not None else None\n",
    "        self.csv_sdf = start_streaming_dataframe() if search_query is not None else None\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.reset()\n",
    "        \n",
    "streams_manager = StreamsManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StreamingQueriesApp to live monitor the progress of the active Streaming Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pixiedust.display.app import *\n",
    "@PixieApp\n",
    "class StreamingQueriesApp():\n",
    "    @route()\n",
    "    def main_screen(self):\n",
    "        return \"\"\"\n",
    "<div class=\"no_loading_msg\" pd_refresh_rate=\"5000\" pd_options=\"show_progress=true\">\n",
    "</div>\n",
    "        \"\"\"\n",
    "        \n",
    "    @route(show_progress=\"true\")\n",
    "    def do_show_progress(self):\n",
    "        return \"\"\"\n",
    "{%for query in this.spark.streams.active%}\n",
    "    <div>\n",
    "    <div class=\"page-header\"> \n",
    "        <h1>Progress Report for Spark Stream: {{query.id}}</h1>\n",
    "    <div>\n",
    "    <table>\n",
    "        <thead>\n",
    "          <tr>\n",
    "             <th>metric</th>\n",
    "             <th>value</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            {%for key, value in query.lastProgress.items()%}\n",
    "            <tr>\n",
    "                <td>{{key}}</td>\n",
    "                <td>{{value}}</td>\n",
    "            </tr>\n",
    "            {%endfor%}\n",
    "        </tbody>        \n",
    "    </table>\n",
    "{%endfor%}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TweetInsightApp shows the metrics in a dashboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@PixieApp\n",
    "class TweetInsightApp():    \n",
    "    @route()\n",
    "    def main_screen(self):\n",
    "        return \"\"\"\n",
    "<style>\n",
    "    div.outer-wrapper {\n",
    "        display: table;width:100%;height:300px;\n",
    "    }\n",
    "    div.inner-wrapper {\n",
    "        display: table-cell;vertical-align: middle;height: 100%;width: 100%;\n",
    "    }\n",
    "</style>\n",
    "<div class=\"outer-wrapper\">\n",
    "    <div class=\"inner-wrapper\">\n",
    "        <div class=\"col-sm-3\"></div>\n",
    "        <div class=\"input-group col-sm-6\">\n",
    "          <input id=\"query{{prefix}}\" type=\"text\" class=\"form-control\"\n",
    "              value=\"\"\n",
    "              placeholder=\"Enter a search query (e.g. baseball)\">\n",
    "          <span class=\"input-group-btn\">\n",
    "            <button class=\"btn btn-default\" type=\"button\" pd_options=\"search_query=$val(query{{prefix}})\">\n",
    "                Go\n",
    "            </button>\n",
    "          </span>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "        \"\"\"\n",
    "    \n",
    "    @route(search_query=\"*\")\n",
    "    def do_search_query(self, search_query):\n",
    "        streams_manager.reset(search_query)\n",
    "        start_parquet_streaming_query(streams_manager.csv_sdf)\n",
    "        while True:\n",
    "            try:\n",
    "                parquet_dir = os.path.join(root_dir, \"output_parquet\")\n",
    "                self.parquet_df = spark.sql(\"select * from parquet.`{}`\".format(parquet_dir))\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(5)\n",
    "        return \"\"\"\n",
    "<div class=\"container\">\n",
    "    <div id=\"header{{prefix}}\" class=\"row no_loading_msg\" pd_refresh_rate=\"5000\" pd_target=\"header{{prefix}}\">\n",
    "        <pd_script>\n",
    "print(\"Number of tweets received: {}\".format(streams_manager.twitter_stream.listener.tweet_count))\n",
    "        </pd_script>\n",
    "    </div>\n",
    "    <div class=\"row\" style=\"min-height:300px\">\n",
    "        <div class=\"col-sm-5\">\n",
    "            <div id=\"metric1{{prefix}}\" pd_refresh_rate=\"10000\" class=\"no_loading_msg\"\n",
    "                pd_options=\"display_metric1=true\" pd_target=\"metric1{{prefix}}\">\n",
    "            </div>\n",
    "        </div>\n",
    "        <div class=\"col-sm-5\">\n",
    "            <div id=\"metric2{{prefix}}\" pd_refresh_rate=\"12000\" class=\"no_loading_msg\"\n",
    "                pd_options=\"display_metric2=true\" pd_target=\"metric2{{prefix}}\">\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"row\" style=\"min-height:400px\">\n",
    "        <div class=\"col-sm-offset-1 col-sm-10\">\n",
    "            <div id=\"word_cloud{{prefix}}\" pd_refresh_rate=\"20000\" class=\"no_loading_msg\"\n",
    "                pd_options=\"display_wc=true\" pd_target=\"word_cloud{{prefix}}\">\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    @route(display_metric1=\"*\")\n",
    "    def do_display_metric1(self, display_metric1):\n",
    "        parquet_dir = os.path.join(root_dir, \"output_parquet\")\n",
    "        self.parquet_df = spark.sql(\"select * from parquet.`{}`\".format(parquet_dir))\n",
    "        return \"\"\"\n",
    "<div class=\"no_loading_msg\" pd_render_onload pd_entity=\"parquet_df\">\n",
    "    <pd_options>\n",
    "    {\n",
    "      \"legend\": \"true\",\n",
    "      \"keyFields\": \"sentiment\",\n",
    "      \"clusterby\": \"entity_type\",\n",
    "      \"handlerId\": \"barChart\",\n",
    "      \"rendererId\": \"bokeh\",\n",
    "      \"rowCount\": \"10\",\n",
    "      \"sortby\": \"Values DESC\",\n",
    "      \"noChartCache\": \"true\"\n",
    "    }\n",
    "    </pd_options>\n",
    "</div>\n",
    "        \"\"\"\n",
    "    \n",
    "    @route(display_metric2=\"*\")\n",
    "    def do_display_metric2(self, display_metric2):\n",
    "        return \"\"\"\n",
    "<div class=\"no_loading_msg\" pd_render_onload pd_entity=\"parquet_df\">\n",
    "    <pd_options>\n",
    "    {\n",
    "      \"keyFields\": \"created_at\",\n",
    "      \"rowCount\": \"1000\",\n",
    "      \"handlerId\": \"lineChart\",\n",
    "      \"clusterby\": \"sentiment\",\n",
    "      \"lineChartType\": \"subplots\",\n",
    "      \"legend\": \"false\",\n",
    "      \"noChartCache\": \"true\"\n",
    "    }\n",
    "    </pd_options>\n",
    "</div>\n",
    "        \"\"\"\n",
    "    \n",
    "    @route(display_wc=\"*\")\n",
    "    @captureOutput\n",
    "    def do_display_wc(self):\n",
    "        text = \"\\n\".join(\n",
    "            [r['entity'] for r in self.parquet_df.select(\"entity\").collect() if r['entity'] is not None]\n",
    "        )\n",
    "        plt.figure( figsize=(13,7) )\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(\n",
    "            WordCloud(width=750, height=350).generate(text), \n",
    "            interpolation='bilinear'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting together the complete PixieApp using TemplateTabbedApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "pixieapp_metadata": null
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pixiedust.display.app import *\n",
    "from pixiedust.apps.template import TemplateTabbedApp\n",
    "\n",
    "@PixieApp\n",
    "class TwitterSentimentApp(TemplateTabbedApp):\n",
    "    def setup(self):\n",
    "        self.apps = [\n",
    "            {\"title\": \"Tweets Insights\", \"app_class\": \"TweetInsightApp\"},\n",
    "            {\"title\": \"Streaming Queries\", \"app_class\": \"StreamingQueriesApp\"}\n",
    "        ]\n",
    "        \n",
    "app = TwitterSentimentApp()\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.3)",
   "language": "python",
   "name": "pythonwithpixiedustspark23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
