{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Part 3\n",
    "## Create a real-time dashboard PixieApp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run the line below to install tweepy if needed\n",
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up twitter authentication\n",
    "Make sure to fill in the tokens below before running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "# Go to http://apps.twitter.com and create an app.\n",
    "# The consumer key and secret will be generated for you after\n",
    "consumer_key=\"XXXX\"\n",
    "consumer_secret=\"XXXX\"\n",
    "\n",
    "# After the step above, you will be redirected to your app's page.\n",
    "# Create an access token under the the \"Your access token\" section\n",
    "access_token=\"XXXX\"\n",
    "access_token_secret=\"XXXX\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a Natural Language Understanding client instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import Features, SentimentOptions, EntitiesOptions\n",
    "\n",
    "nlu = NaturalLanguageUnderstandingV1(\n",
    "    version='2017-02-27',\n",
    "    username='XXXX',\n",
    "    password='XXXX'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Twitter Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.9</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from pixiedust.utils import Logger\n",
    "from tweepy import Stream\n",
    "from six import iteritems\n",
    "import json\n",
    "import csv\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, TimestampType\n",
    "\n",
    "def ensure_dir(dir, delete_tree = False):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    elif delete_tree:\n",
    "        shutil.rmtree(dir)\n",
    "        os.makedirs(dir)\n",
    "    return os.path.abspath(dir)\n",
    "\n",
    "def init_output_dirs():\n",
    "    root_dir = ensure_dir(\"output\", delete_tree = True)\n",
    "    output_dir = ensure_dir(os.path.join(root_dir, \"raw\"))\n",
    "    return (root_dir, output_dir)\n",
    "    \n",
    "root_dir, output_dir = init_output_dirs()\n",
    "\n",
    "field_metadata = [\n",
    "    {\"name\": \"created_at\",\"type\": TimestampType()},\n",
    "    {\"name\": \"text\", \"type\": StringType()},\n",
    "    {\"name\": \"source\", \"type\": StringType(), \n",
    "         \"transform\": lambda s: BS(s, \"html.parser\").text.strip()\n",
    "    },\n",
    "    {\"name\": \"sentiment\", \"type\": StringType()},\n",
    "    {\"name\": \"entity\", \"type\": StringType()},\n",
    "    {\"name\": \"entity_type\", \"type\": StringType()}\n",
    "]\n",
    "fieldnames = [f[\"name\"] for f in field_metadata]\n",
    "transforms = { \n",
    "    item['name']:item['transform'] for item in field_metadata if \"transform\" in item\n",
    "}\n",
    "\n",
    "@Logger()\n",
    "class RawTweetsListener(StreamListener):\n",
    "    def __init__(self):\n",
    "        self.buffered_data = []\n",
    "        self.counter = 0\n",
    "        self.tweet_count = 0\n",
    "\n",
    "    def flush_buffer_if_needed(self):\n",
    "        \"Check the buffer capacity and write to a new file if needed\"\n",
    "        length = len(self.buffered_data)\n",
    "        if length > 0 and length % 10 == 0:\n",
    "            with open(os.path.join( output_dir, \"tweets{}.csv\".format(self.counter)), \"w\") as fs:\n",
    "                self.counter += 1\n",
    "                if self.counter % 20 == 0:\n",
    "                    self.counter = 0\n",
    "                csv_writer = csv.DictWriter( fs, fieldnames = fieldnames)\n",
    "                for data in self.buffered_data:\n",
    "                    csv_writer.writerow(data)\n",
    "            self.buffered_data = []\n",
    "            \n",
    "    def enrich(self, data):\n",
    "        try:\n",
    "            data['text'] = data['text'].replace('\"', \"'\")\n",
    "            response = nlu.analyze( \n",
    "                text = data['text'],\n",
    "                features=Features(sentiment=SentimentOptions(), entities=EntitiesOptions())\n",
    "            )\n",
    "            data[\"sentiment\"] = response[\"sentiment\"][\"document\"][\"label\"]\n",
    "            top_entity = response[\"entities\"][0] if len(response[\"entities\"]) > 0 else None\n",
    "            data[\"entity\"] = top_entity[\"text\"] if top_entity is not None else \"\"\n",
    "            data[\"entity_type\"] = top_entity[\"type\"] if top_entity is not None else \"\"\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            self.warn(\"Error from Watson service while enriching data: {}\".format(e))\n",
    "\n",
    "    def on_data(self, data):\n",
    "        def transform(key, value):\n",
    "            return transforms[key](value) if key in transforms else value\n",
    "        data = self.enrich(json.loads(data))\n",
    "        if data is not None:\n",
    "            self.tweet_count += 1\n",
    "            self.buffered_data.append(\n",
    "                {key:transform(key,value) \\\n",
    "                     for key,value in iteritems(data) \\\n",
    "                     if key in fieldnames}\n",
    "            )\n",
    "            self.flush_buffer_if_needed()\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(\"An error occured while receiving streaming data: {}\".format(status))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_stream(queries):\n",
    "    \"Asynchronously start a new Twitter stream\"\n",
    "    stream = Stream(auth, RawTweetsListener())\n",
    "    stream.filter(track=queries, languages=[\"en\"], async=True)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark Streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def start_streaming_dataframe(output_dir):\n",
    "    \"Start a Spark Streaming DataFrame from a file source\"\n",
    "    schema = StructType(\n",
    "        [StructField(f[\"name\"], f[\"type\"], True) for f in field_metadata]\n",
    "    )\n",
    "    return spark.readStream \\\n",
    "        .csv(\n",
    "            output_dir,\n",
    "            schema=schema,\n",
    "            multiLine = True,\n",
    "            timestampFormat = 'EEE MMM dd kk:mm:ss Z yyyy',\n",
    "            ignoreTrailingWhiteSpace = True,\n",
    "            ignoreLeadingWhiteSpace = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Run Spark Structured Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_parquet_streaming_query(csv_sdf):\n",
    "    \"\"\"\n",
    "    Create an run a streaming query from a Structured DataFrame \n",
    "    outputing the results into a parquet database\n",
    "    \"\"\"\n",
    "    streaming_query = csv_sdf \\\n",
    "      .writeStream \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", os.path.join(root_dir, \"output_parquet\")) \\\n",
    "      .trigger(processingTime=\"2 seconds\") \\\n",
    "      .option(\"checkpointLocation\", os.path.join(root_dir, \"output_chkpt\")) \\\n",
    "      .start()\n",
    "    return streaming_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a real-time dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StreamsManager class for controlling the lifecycle of the different streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamsManager():\n",
    "    def __init__(self):\n",
    "        self.twitter_stream = None\n",
    "        self.csv_sdf = None\n",
    "        \n",
    "    def reset(self, search_query = None):\n",
    "        if self.twitter_stream is not None:\n",
    "            self.twitter_stream.disconnect()\n",
    "        #stop all the active streaming queries and re_initialize the directories\n",
    "        for query in spark.streams.active:\n",
    "            query.stop()\n",
    "        self.root_dir, self.output_dir = init_output_dirs()\n",
    "        self.twitter_stream = start_stream([search_query]) if search_query is not None else None\n",
    "        self.csv_sdf = start_streaming_dataframe(output_dir) if search_query is not None else None\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.reset()\n",
    "        \n",
    "streams_manager = StreamsManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StreamingQueriesApp to live monitor the progress of the active Streaming Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixiedust.display.app import *\n",
    "@PixieApp\n",
    "class StreamingQueriesApp():\n",
    "    @route()\n",
    "    def main_screen(self):\n",
    "        return \"\"\"\n",
    "<div class=\"no_loading_msg\" pd_refresh_rate=\"5000\" pd_options=\"show_progress=true\">\n",
    "</div>\n",
    "        \"\"\"\n",
    "        \n",
    "    @route(show_progress=\"true\")\n",
    "    def do_show_progress(self):\n",
    "        return \"\"\"\n",
    "{%for query in this.spark.streams.active%}\n",
    "    <div>\n",
    "    <div class=\"page-header\"> \n",
    "        <h1>Progress Report for Spark Stream: {{query.id}}</h1>\n",
    "    <div>\n",
    "    <table>\n",
    "        <thead>\n",
    "          <tr>\n",
    "             <th>metric</th>\n",
    "             <th>value</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            {%for key, value in query.lastProgress.items()%}\n",
    "            <tr>\n",
    "                <td>{{key}}</td>\n",
    "                <td>{{value}}</td>\n",
    "            </tr>\n",
    "            {%endfor%}\n",
    "        </tbody>        \n",
    "    </table>\n",
    "{%endfor%}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TweetInsightApp shows the metrics in a dashboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@PixieApp\n",
    "class TweetInsightApp():    \n",
    "    @route()\n",
    "    def main_screen(self):\n",
    "        return \"\"\"\n",
    "<style>\n",
    "    div.outer-wrapper {\n",
    "        display: table;width:100%;height:300px;\n",
    "    }\n",
    "    div.inner-wrapper {\n",
    "        display: table-cell;vertical-align: middle;height: 100%;width: 100%;\n",
    "    }\n",
    "</style>\n",
    "<div class=\"outer-wrapper\">\n",
    "    <div class=\"inner-wrapper\">\n",
    "        <div class=\"col-sm-3\"></div>\n",
    "        <div class=\"input-group col-sm-6\">\n",
    "          <input id=\"query{{prefix}}\" type=\"text\" class=\"form-control\"\n",
    "              value=\"\"\n",
    "              placeholder=\"Enter a search query (e.g. baseball)\">\n",
    "          <span class=\"input-group-btn\">\n",
    "            <button class=\"btn btn-default\" type=\"button\" pd_options=\"search_query=$val(query{{prefix}})\">\n",
    "                Go\n",
    "            </button>\n",
    "          </span>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "        \"\"\"\n",
    "    \n",
    "    @route(search_query=\"*\")\n",
    "    def do_search_query(self, search_query):\n",
    "        streams_manager.reset(search_query)\n",
    "        start_parquet_streaming_query(streams_manager.csv_sdf)\n",
    "        while True:\n",
    "            try:\n",
    "                parquet_dir = os.path.join(root_dir, \"output_parquet\")\n",
    "                self.parquet_df = spark.sql(\"select * from parquet.`{}`\".format(parquet_dir))\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(5)\n",
    "        return \"\"\"\n",
    "<div class=\"container\">\n",
    "    <div id=\"header{{prefix}}\" class=\"row no_loading_msg\" pd_refresh_rate=\"5000\" pd_target=\"header{{prefix}}\">\n",
    "        <pd_script>\n",
    "print(\"Number of tweets received: {}\".format(streams_manager.twitter_stream.listener.tweet_count))\n",
    "        </pd_script>\n",
    "    </div>\n",
    "    <div class=\"row\" style=\"min-height:300px\">\n",
    "        <div class=\"col-sm-5\">\n",
    "            <div id=\"metric1{{prefix}}\" pd_refresh_rate=\"10000\" class=\"no_loading_msg\"\n",
    "                pd_options=\"display_metric1=true\" pd_target=\"metric1{{prefix}}\">\n",
    "            </div>\n",
    "        </div>\n",
    "        <div class=\"col-sm-5\">\n",
    "            <div id=\"metric2{{prefix}}\" pd_refresh_rate=\"12000\" class=\"no_loading_msg\"\n",
    "                pd_options=\"display_metric2=true\" pd_target=\"metric2{{prefix}}\">\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"row\" style=\"min-height:400px\">\n",
    "        <div class=\"col-sm-offset-1 col-sm-10\">\n",
    "            <div id=\"word_cloud{{prefix}}\" pd_refresh_rate=\"20000\" class=\"no_loading_msg\"\n",
    "                pd_options=\"display_wc=true\" pd_target=\"word_cloud{{prefix}}\">\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    @route(display_metric1=\"*\")\n",
    "    def do_display_metric1(self, display_metric1):\n",
    "        parquet_dir = os.path.join(root_dir, \"output_parquet\")\n",
    "        self.parquet_df = spark.sql(\"select * from parquet.`{}`\".format(parquet_dir))\n",
    "        return \"\"\"\n",
    "<div class=\"no_loading_msg\" pd_render_onload pd_entity=\"parquet_df\">\n",
    "    <pd_options>\n",
    "    {\n",
    "      \"legend\": \"true\",\n",
    "      \"keyFields\": \"sentiment\",\n",
    "      \"clusterby\": \"entity_type\",\n",
    "      \"handlerId\": \"barChart\",\n",
    "      \"rendererId\": \"bokeh\",\n",
    "      \"rowCount\": \"10\",\n",
    "      \"sortby\": \"Values DESC\",\n",
    "      \"noChartCache\": \"true\"\n",
    "    }\n",
    "    </pd_options>\n",
    "</div>\n",
    "        \"\"\"\n",
    "    \n",
    "    @route(display_metric2=\"*\")\n",
    "    def do_display_metric2(self, display_metric2):\n",
    "        return \"\"\"\n",
    "<div class=\"no_loading_msg\" pd_render_onload pd_entity=\"parquet_df\">\n",
    "    <pd_options>\n",
    "    {\n",
    "      \"keyFields\": \"created_at\",\n",
    "      \"rowCount\": \"1000\",\n",
    "      \"handlerId\": \"lineChart\",\n",
    "      \"clusterby\": \"sentiment\",\n",
    "      \"lineChartType\": \"subplots\",\n",
    "      \"legend\": \"false\",\n",
    "      \"noChartCache\": \"true\"\n",
    "    }\n",
    "    </pd_options>\n",
    "</div>\n",
    "        \"\"\"\n",
    "    \n",
    "    @route(display_wc=\"*\")\n",
    "    @captureOutput\n",
    "    def do_display_wc(self):\n",
    "        text = \"\\n\".join(\n",
    "            [r['entity'] for r in self.parquet_df.select(\"entity\").collect() if r['entity'] is not None]\n",
    "        )\n",
    "        plt.figure( figsize=(13,7) )\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(\n",
    "            WordCloud(width=750, height=350).generate(text), \n",
    "            interpolation='bilinear'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting together the complete PixieApp using TemplateTabbedApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pixiedust.display.app import *\n",
    "from pixiedust.apps.template import TemplateTabbedApp\n",
    "\n",
    "@PixieApp\n",
    "class TwitterSentimentApp(TemplateTabbedApp):\n",
    "    def setup(self):\n",
    "        self.apps = [\n",
    "            {\"title\": \"Tweets Insights\", \"app_class\": \"TweetInsightApp\"},\n",
    "            {\"title\": \"Streaming Queries\", \"app_class\": \"StreamingQueriesApp\"}\n",
    "        ]\n",
    "        \n",
    "app = TwitterSentimentApp()\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.3)",
   "language": "python",
   "name": "pythonwithpixiedustspark23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
